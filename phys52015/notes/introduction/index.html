<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Introduction #  This course provides a brief introduction to parallel computing. It focuses on those paradigms that are prevalent in scientific computing in academia. There are other parallel programming models which we will mention in passing where appropriate.
Task  Sign up for a Hamilton account if you don&rsquo;t already have one. If you have a COSMA account, you can use that instead, and need not sign up for Hamilton access (although you may wish to anyway)."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="Introduction and motivation"><meta property="og:description" content="Introduction #  This course provides a brief introduction to parallel computing. It focuses on those paradigms that are prevalent in scientific computing in academia. There are other parallel programming models which we will mention in passing where appropriate.
Task  Sign up for a Hamilton account if you don&rsquo;t already have one. If you have a COSMA account, you can use that instead, and need not sign up for Hamilton access (although you may wish to anyway)."><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/phys52015/notes/introduction/"><meta property="article:modified_time" content="2020-10-02T19:16:20+01:00"><title>Introduction and motivation | PHYS52015 – Introduction to HPC</title><link rel=manifest href=/phys52015/manifest.json><link rel=icon href=/phys52015/favicon.png type=image/x-icon><link rel=stylesheet href=/phys52015/book.min.bf7f3e732f1a68bc04f2ce50cc8c8a462404f04066d1e404f002f2914662d55d.css integrity="sha256-v38+cy8aaLwE8s5QzIyKRiQE8EBm0eQE8ALykUZi1V0="></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/phys52015/logo.svg alt=Logo><h2><a href=/phys52015>PHYS52015 – Introduction to HPC</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/phys52015/setup/hamilton-quickstart/>Hamilton access & quickstart</a></li><li><a href=/phys52015/setup/byod/>Local setup</a></li><li><a href=/phys52015/setup/configuration/>ssh configuration</a></li><li><a href=/phys52015/setup/unix/>Unix resources</a></li></ul></li><li><span>Exercises</span><ul><li><a href=/phys52015/exercises/hello/>Parallel Hello World</a></li><li><a href=/phys52015/exercises/vectorisation-loop/>Vectorisation: loops with conditionals</a></li><li><a href=/phys52015/exercises/vectorisation-stencil/>Vectorisation: stencils</a></li><li><a href=/phys52015/exercises/openmp-loop/>OpenMP: parallel loops</a></li><li><a href=/phys52015/exercises/openmp-stencil/>OpenMP: stencils & profiling</a></li><li><a href=/phys52015/exercises/mpi-pi/>MPI: Calculating π</a></li><li><a href=/phys52015/exercises/mpi-stencil/>MPI: halo exchanges</a></li></ul></li><li><span>Notes</span><ul><li><a href=/phys52015/notes/introduction/ class=active>Introduction and motivation</a></li><li><span>Theory & concepts</span><ul><li><a href=/phys52015/notes/theory/scaling-laws/>Parallel scaling laws</a></li><li><a href=/phys52015/notes/theory/hardware-parallelism/>Parallelism in hardware: an overview</a></li><li><a href=/phys52015/notes/theory/concepts/>Parallel patterns</a></li></ul></li><li><a href=/phys52015/notes/vectorisation/>Vectorisation</a><ul></ul></li><li><a href=/phys52015/notes/openmp/>OpenMP</a><ul><li><a href=/phys52015/notes/openmp/openmp-collectives/>Collectives</a></li><li><a href=/phys52015/notes/openmp/openmp-loop-parallelism/>Loop parallelism</a></li><li><a href=/phys52015/notes/openmp/openmp-tasks/>Task parallelism</a></li></ul></li><li><a href=/phys52015/notes/mpi/>MPI</a><ul><li><a href=/phys52015/notes/mpi/mpi-ptp/>Point-to-point messaging</a></li><li><a href=/phys52015/notes/mpi/mpi-collectives/>Collectives</a></li><li><a href=/phys52015/notes/mpi/mpi-advanced/>Advanced topics</a></li></ul></li></ul></li><li><a href=/phys52015/resources/>Further resources</a></li><li><a href=/phys52015/acknowledgements/>Acknowledgements</a></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/phys52015/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Introduction and motivation</strong>
<label for=toc-control></label></div></header><article class=markdown><h1 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h1><p>This course provides a brief introduction to parallel computing. It
focuses on those paradigms that are prevalent in scientific computing
in academia. There are other parallel programming models which we will
mention in passing where appropriate.</p><blockquote class=task><h3>Task</h3><span>Sign up for a <a href=https://teaching.wence.uk/phys52015/setup/hamilton-quickstart/>Hamilton account</a>
if you don&rsquo;t already have one. If you have a COSMA account, you can
use that instead, and need not sign up for Hamilton access (although
you may wish to anyway).</span></blockquote><h2 id=why-parallel-computing>Why parallel computing?
<a class=anchor href=#why-parallel-computing>#</a></h2><p>Computing in parallel is <em>more difficult</em> than computing in serial. We
often have to write more code, or use more complicated libraries. If
we are going to commit to doing it, there must be some advantage.</p><p>Our goal when running scientific simulations or data analyses is often
to increase the resolution of the simulation, run more
replicates (for example more bootstrapping samples in a statistical
test), obtain answers faster, or some combination thereof.</p><p>In some cases, we might be analysing or simulating such
high-resolution data that it does not fit in the memory of a single
computer. Or, the problem may take so long that running it one a
single computer would require us to wait a week or longer for the
result.</p><p>For example, the <a href=https://www.metoffice.gov.uk>UK Met Office</a>
produces a weather forecast for the UK six times every day. That is,
every 4 hours, they run a computer model which predicts the weather
for the next three days<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Obviously, this forecast would be useless
if the computer model took longer than 4 hours to run! The model over
the UK is at a <a href=https://www.metoffice.gov.uk/research/approach/modelling-systems/unified-model/weather-forecasting>resolution of
1.5km</a>.
This produces a computer problem that is large enough that each run is
parallelised across 500 compute cores (so that they can hit their
operational window of taking less than one hour to produce a
forecast). To do this, the data, and algorithmic work that produce the
model answers must be split up across these processes.</p><p>A perhaps simpler example comes when performing various forms of Monte
Carlo integration. Of which we will see a very simple example in <a href=https://teaching.wence.uk/phys52015/exercises/mpi-pi/>one
of the exercises</a>. The accuracy of Monte
Carlo estimators increases when we add more samples. These samples are
often completely independent. If we want to generate more samples (to
improve accuracy), we can do so by running a single process code for
longer, or dividing up the work of generating samples among multiple
processes.</p><h2 id=moores-law><a href=https://en.wikipedia.org/wiki/Moore%27s_law>Moore&rsquo;s Law</a>
<a class=anchor href=#moores-law>#</a></h2><blockquote><p>The number of transistors in an integrated circuit doubles about
every two years.</p></blockquote><p>This was an observation Gordon Moore made about the <em>complexity</em> of
chips in the
<a href=https://newsroom.intel.com/wp-content/uploads/sites/11/2018/05/moores-law-electronics.pdf>mid-60s</a>.
That is, every 18-24 months, the computational
performance available at a fixed price doubles.</p><p>This trend can be observed when we look at the performance of systems
in the <a href=https://www.top500.org>Top 500</a>, which has been measuring the
floating point performance of the largest 500 supercomputers in the
world since 1993.</p><figure><img src=https://teaching.wence.uk/phys52015/images/auto/top500.svg alt="Performance trends of the Top500 list since inception." width=50%><figcaption><p>Performance trends of the Top500 list since inception.</p></figcaption></figure><p>This seems great, however, there is a catch. If we zoom in and look
at a history of CPU trends we see a slightly different picture.</p><a href=https://github.com/karlrupp/microprocessor-trend-data><figure style=width:100%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/48-years-processor-trend.png alt="48 years of microprocessor trend data (image by Karl Rupp)"><figcaption><p>48 years of microprocessor trend data (image by Karl Rupp)</p></figcaption></figure></a><p>Although the transistor counts still follow an exponential growth
curve, CPU clock speed stopped increasing in around 2004. As a
consequence, the <em>single thread</em> (SpecINT in the above figure)
performance of modern chips has almost flat-lined since 2008. This
Herb Sutter makes some nice observations about this in <a href=http://www.gotw.ca/publications/concurrency-ddj.htm><em>The Free
Lunch is Over</em></a>.</p><blockquote class=exercise><h3>Exercise</h3><span>Read <a href=http://www.gotw.ca/publications/concurrency-ddj.htm><em>The Free Lunch is
Over</em></a>. Make a
note of what you think the key points are, and anything you didn&rsquo;t
understand, to bring to the live sessions.</span></blockquote><h2 id=consequences>Consequences
<a class=anchor href=#consequences>#</a></h2><p>If the number of transistors continues to increase, but the effective
single-thread performance of CPUs does not (or at least, does not at
the same rate), what are all these extra transistors being used for?</p><p>The answer is that they are used to provide parallelism. Notice how
since 2004, the number of logical cores on a CPU has risen from one to
typically around 32.</p><p>The consequence for us as programmers is that to use all the performance
that chips provide, we must write code that exposes and uses
parallelism.</p><h2 id=where-next>Where next?
<a class=anchor href=#where-next>#</a></h2><p>We&rsquo;ll introduce the different types of parallelism offered by modern
supercomputers, and look at potential programming models and
parallelisation strategies.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>This is a slight simplification, see their <a href=https://www.metoffice.gov.uk/research/approach/modelling-systems/unified-model/weather-forecasting>detailed
breakdown</a>
for the full picture. <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/commit/7df2b6e5195d06bf46e068ff68288326999505ee title="Last modified by Lawrence Mitchell | October 2, 2020" target=_blank rel=noopener><img src=/phys52015/svg/calendar.svg class=book-icon alt=Calendar>
<span>October 2, 2020</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/edit/main/site/content/notes/introduction.md target=_blank rel=noopener><img src=/phys52015/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence@wence.uk>Lawrence Mitchell</a>, <a href="https://www.dur.ac.uk/physics/staff/profiles/?mode=staff&id=16712">Christian Arnold</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/phys52015/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div></main></body></html>