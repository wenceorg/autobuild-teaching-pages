<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="OpenMP #  Recall that one of the levels of parallelism offered by computer hardware is shared memory parallelism. In this style of computer architecture, there are multiple CPU cores that share main memory.
 Sketch of a shared memory chip with four sockets.
  In this picture, any CPU can access any of the memory.
Communication, and therefore coordination, between CPUs happens by reading and writing to this shared memory."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="OpenMP"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://teaching.wence.uk/phys52015/notes/openmp/"><title>OpenMP | PHYS52015 – Introduction to HPC</title><link rel=manifest href=/phys52015/manifest.json><link rel=icon href=/phys52015/favicon.png type=image/x-icon><link rel=stylesheet href=/phys52015/book.min.bf7f3e732f1a68bc04f2ce50cc8c8a462404f04066d1e404f002f2914662d55d.css integrity="sha256-v38+cy8aaLwE8s5QzIyKRiQE8EBm0eQE8ALykUZi1V0="></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/phys52015/logo.svg alt=Logo><h2><a href=/phys52015>PHYS52015 – Introduction to HPC</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/phys52015/setup/hamilton-quickstart/>Hamilton access & quickstart</a></li><li><a href=/phys52015/setup/byod/>Local setup</a></li><li><a href=/phys52015/setup/configuration/>ssh configuration</a></li><li><a href=/phys52015/setup/unix/>Unix resources</a></li></ul></li><li><span>Exercises</span><ul><li><a href=/phys52015/exercises/hello/>Parallel Hello World</a></li><li><a href=/phys52015/exercises/vectorisation-loop/>Vectorisation: loops with conditionals</a></li><li><a href=/phys52015/exercises/vectorisation-stencil/>Vectorisation: stencils</a></li><li><a href=/phys52015/exercises/openmp-loop/>OpenMP: parallel loops</a></li><li><a href=/phys52015/exercises/openmp-stencil/>OpenMP: stencils & profiling</a></li><li><a href=/phys52015/exercises/mpi-pi/>MPI: Calculating π</a></li><li><a href=/phys52015/exercises/mpi-stencil/>MPI: halo exchanges</a></li></ul></li><li><span>Notes</span><ul><li><a href=/phys52015/notes/introduction/>Introduction and motivation</a></li><li><span>Theory & concepts</span><ul><li><a href=/phys52015/notes/theory/scaling-laws/>Parallel scaling laws</a></li><li><a href=/phys52015/notes/theory/hardware-parallelism/>Parallelism in hardware: an overview</a></li><li><a href=/phys52015/notes/theory/concepts/>Parallel patterns</a></li></ul></li><li><a href=/phys52015/notes/vectorisation/>Vectorisation</a><ul><li><a href=/phys52015/notes/vectorisation/compiler/>Compiler autovectorisation</a></li></ul></li><li><a href=/phys52015/notes/openmp/ class=active>OpenMP</a><ul><li><a href=/phys52015/notes/openmp/intro/>What is OpenMP?</a></li><li><a href=/phys52015/notes/openmp/loop-parallelism/>Loop parallelism</a></li><li><a href=/phys52015/notes/openmp/collectives/>Collectives</a></li><li><a href=/phys52015/notes/openmp/tasks/>Task parallelism</a></li></ul></li><li><a href=/phys52015/notes/mpi/>MPI</a><ul><li><a href=/phys52015/notes/mpi/mpi-ptp/>Point-to-point messaging</a></li><li><a href=/phys52015/notes/mpi/mpi-collectives/>Collectives</a></li><li><a href=/phys52015/notes/mpi/mpi-advanced/>Advanced topics</a></li></ul></li></ul></li><li><a href=/phys52015/coursework/>Coursework: parallel dense linear algebra</a><ul></ul></li><li><a href=/phys52015/resources/>Further resources</a></li><li><a href=/phys52015/acknowledgements/>Acknowledgements</a></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/phys52015/svg/menu.svg class=book-icon alt=Menu></label>
<strong>OpenMP</strong>
<label for=toc-control></label></div></header><article class=markdown><h1 id=openmp>OpenMP
<a class=anchor href=#openmp>#</a></h1><p>Recall that one of the levels of parallelism offered by computer
hardware is <a href=https://teaching.wence.uk/phys52015/notes/theory/hardware-parallelism/#shared-memory>shared memory</a> parallelism. In this
style of computer architecture, there are <em>multiple</em> CPU cores that
share main memory.</p><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/numa-socket-sketch.svg alt="Sketch of a shared memory chip with four sockets."><figcaption><p>Sketch of a shared memory chip with four sockets.</p></figcaption></figure><p>In this picture, any CPU can access any of the memory.</p><p>Communication, and therefore coordination, between CPUs happens by
reading and writing to this shared memory.</p><p>Equally, division of work between CPUs can be arranged by each CPU
deciding which part of the data to work on.</p><p>Let us consider a cartoon of how this might work with two CPUs
cooperating to add up all the entries in a vector. The serial code
looks like this</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>double</span> <span style=color:#75af00>sum_vector</span><span style=color:#111>(</span><span style=color:#00a8c8>double</span> <span style=color:#f92672>*</span><span style=color:#111>a</span><span style=color:#111>,</span> <span style=color:#00a8c8>int</span> <span style=color:#111>N</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>double</span> <span style=color:#111>sum</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span>
  <span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span>
    <span style=color:#111>sum</span> <span style=color:#f92672>=</span> <span style=color:#111>sum</span> <span style=color:#f92672>+</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>];</span>
  <span style=color:#00a8c8>return</span> <span style=color:#111>sum</span><span style=color:#111>;</span>
<span style=color:#111>}</span>
</code></pre></div><p>In parallel, the obvious thing to do is to divide up the array into
two halves</p><div class="book-columns flex flex-wrap"><div class="flex-even markdown-inner"><p>CPU 0</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#f92672>/</span><span style=color:#ae81ff>2</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span>
  <span style=color:#111>sum0</span> <span style=color:#f92672>=</span> <span style=color:#111>sum0</span> <span style=color:#f92672>+</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>];</span>
</code></pre></div></div><div class="flex-even markdown-inner"><p>CPU 1</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#111>N</span><span style=color:#f92672>/</span><span style=color:#ae81ff>2</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span>
  <span style=color:#111>sum1</span> <span style=color:#f92672>=</span> <span style=color:#111>sum1</span> <span style=color:#f92672>+</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>];</span>
</code></pre></div></div></div><p>These two <em>partial</em> sums must then be merged into a single result.</p><h2 id=sync-data-race>Synchronisation and data races
<a class=anchor href=#sync-data-race>#</a></h2><p>Conceptually this seems very straightforward, however there are some
subtleties that we should be aware of. The major one is to note that
the partial sums write to separate variables (and hence separate
places in memory).</p><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/SM-vector-sum.svg alt="Two CPUs coordinating to produce partial sums of an array."><figcaption><p>Two CPUs coordinating to produce partial sums of an array.</p></figcaption></figure><p>So far, everything is great. When we come to produce the final result,
however, we have a (potential) problem.</p><figure style=width:75%><img class=scaled src=https://teaching.wence.uk/phys52015/images/manual/SM-vector-sum-write-contention.svg alt="Two CPUs might race on writing the final answer."><figcaption><p>Two CPUs might race on writing the final answer.</p></figcaption></figure><p>Since we haven&rsquo;t asked for any <em>synchronisation</em> between the CPUs, if
we naively get the individual CPUs to increment the global summation
result by their partial sum, we are not guaranteed to get the correct
answer (namely <code>sum0 + sum1</code>). The three possible outcomes for <code>sum</code>
are:</p><ol><li><code>sum = sum0 + sum1</code>, the correct answer, great!</li><li><code>sum = sum0</code>, oh no.</li><li><code>sum = sum1</code>, also wrong.</li></ol><p>The reason is that to perform the addition both CPUs first load the
current value of <code>sum</code> from memory, increment it, and then store their
answer back to main memory. If <code>sum</code> is initially zero, then they
might both load zero, add their part, and then write back. Whichever
is &ldquo;slower&rdquo; then wins.</p><p>This is an example of a <em>data race</em>, and is the cause of most bugs in
shared memory programming.</p><h2 id=programming-with-shared-memory>Programming with shared memory
<a class=anchor href=#programming-with-shared-memory>#</a></h2><p>The operating system level object that facilitates shared memory
programming is a <em>thread</em>. Threads are like processes (e.g. your text
editor and web browser are two different processes), but they
additionally can share memory (as well as having private,
thread-specific, memory). Threads logically all belong to the same
program, but they can follow different control-flow through the
program (since they each have their own, private, program counter).</p><p>Usually, though not always, we use one thread per physical CPU core.
However, this is not required.</p><p>The other concept is that of a <em>task</em>, which we can describe as some
piece of computation that can be executed independently of, and hence
potentially in parallel with, other tasks.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/commit/3bb8079425fa18149cf7639c7d9b8e5a00e05d74 title="Last modified by Lawrence Mitchell | October 28, 2020" target=_blank rel=noopener><img src=/phys52015/svg/calendar.svg class=book-icon alt=Calendar>
<span>October 28, 2020</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/edit/main/site/content/notes/openmp/_index.md target=_blank rel=noopener><img src=/phys52015/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence@wence.uk>Lawrence Mitchell</a>, <a href="https://www.dur.ac.uk/physics/staff/profiles/?mode=staff&id=16712">Christian Arnold</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/phys52015/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div></main></body></html>