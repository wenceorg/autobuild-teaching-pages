<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Measuring point-to-point message latency with ping-pong #  In this exercise we will write a simple code that does a message ping-pong: sending a message back and forth between two processes.
We can use this to measure both the latency and bandwidth of the network on our supercomputer. Which are both important measurements when we&rsquo;re looking at potential parallel performance: they help us to decide if our code is running slowly because of our bad choices, or limitations in the hardware."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="MPI: ping-pong latency"><meta property="og:description" content="Measuring point-to-point message latency with ping-pong #  In this exercise we will write a simple code that does a message ping-pong: sending a message back and forth between two processes.
We can use this to measure both the latency and bandwidth of the network on our supercomputer. Which are both important measurements when we&rsquo;re looking at potential parallel performance: they help us to decide if our code is running slowly because of our bad choices, or limitations in the hardware."><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/phys52015/exercises/mpi-ping-pong/"><meta property="article:modified_time" content="2020-12-02T19:23:10+00:00"><title>MPI: ping-pong latency | PHYS52015 – Introduction to HPC</title><link rel=manifest href=/phys52015/manifest.json><link rel=icon href=/phys52015/favicon.png type=image/x-icon><link rel=stylesheet href=/phys52015/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"$",right:"$",display:false},{left:"\\(",right:"\\)",display:false},{left:"\\[",right:"\\]",display:true}]})});</script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/phys52015/logo.svg alt=Logo><h2><a href=/phys52015>PHYS52015 – Introduction to HPC</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/phys52015/setup/remote/>Remote editing/development</a></li><li><a href=/phys52015/setup/hamilton-quickstart/>Hamilton access & quickstart</a></li><li><a href=/phys52015/setup/byod/>Local setup</a></li><li><a href=/phys52015/setup/configuration/>ssh configuration</a></li><li><a href=/phys52015/setup/unix/>Unix resources</a></li></ul></li><li><span>Exercises</span><ul><li><a href=/phys52015/exercises/hello/>Parallel Hello World</a></li><li><a href=/phys52015/exercises/vectorisation-loop/>Vectorisation: loops with conditionals</a></li><li><a href=/phys52015/exercises/vectorisation-stencil/>Vectorisation: stencils</a></li><li><a href=/phys52015/exercises/openmp-loop/>OpenMP: parallel loops</a></li><li><a href=/phys52015/exercises/openmp-stencil/>OpenMP: stencils</a></li><li><a href=/phys52015/exercises/openmp-reduction/>OpenMP: synchronisation</a></li><li><a href=/phys52015/exercises/mpi-ring/>MPI: messages round a ring</a></li><li><a href=/phys52015/exercises/mpi-pi/>MPI: Calculating π</a></li><li><a href=/phys52015/exercises/mpi-ping-pong/ class=active>MPI: ping-pong latency</a></li><li><a href=/phys52015/exercises/mpi-collectives/>MPI: simple collectives</a></li><li><a href=/phys52015/exercises/mpi-stencil/>MPI: domain decomposition and halo exchanges</a></li></ul></li><li><span>Notes</span><ul><li><a href=/phys52015/notes/introduction/>Introduction and motivation</a></li><li><span>Theory & concepts</span><ul><li><a href=/phys52015/notes/theory/scaling-laws/>Parallel scaling laws</a></li><li><a href=/phys52015/notes/theory/hardware-parallelism/>Parallelism in hardware: an overview</a></li><li><a href=/phys52015/notes/theory/concepts/>Parallel patterns</a></li></ul></li><li><a href=/phys52015/notes/vectorisation/>Vectorisation</a><ul><li><a href=/phys52015/notes/vectorisation/compiler/>Compiler autovectorisation</a></li></ul></li><li><a href=/phys52015/notes/openmp/>OpenMP</a><ul><li><a href=/phys52015/notes/openmp/intro/>What is OpenMP?</a></li><li><a href=/phys52015/notes/openmp/loop-parallelism/>Loop parallelism</a></li><li><a href=/phys52015/notes/openmp/collectives/>Collectives</a></li></ul></li><li><a href=/phys52015/notes/mpi/>MPI</a><ul><li><a href=/phys52015/notes/mpi/point-to-point/>Point-to-point messaging in MPI</a></li><li><a href=/phys52015/notes/mpi/point-to-point-nb/>Non-blocking point-to-point messaging</a></li><li><a href=/phys52015/notes/mpi/collectives/>Collectives</a></li><li><a href=/phys52015/notes/mpi/advanced/>Advanced topics</a></li></ul></li></ul></li><li><a href=/phys52015/coursework/>Coursework: parallel dense linear algebra</a><ul></ul></li><li><a href=/phys52015/resources/>Further resources</a></li><li><a href=/phys52015/acknowledgements/>Acknowledgements</a></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/phys52015/svg/menu.svg class=book-icon alt=Menu></label>
<strong>MPI: ping-pong latency</strong>
<label for=toc-control></label></div></header><article class=markdown><h1 id=measuring-point-to-point-message-latency-with-ping-pong>Measuring point-to-point message latency with ping-pong
<a class=anchor href=#measuring-point-to-point-message-latency-with-ping-pong>#</a></h1><p>In this exercise we will write a simple code that does a message
ping-pong: sending a message back and forth between two processes.</p><p>We can use this to measure both the <em>latency</em> and <em>bandwidth</em> of the
network on our supercomputer. Which are both important measurements
when we&rsquo;re looking at potential parallel performance: they help us to
decide if our code is running slowly because of our bad choices, or
limitations in the hardware.</p><h2 id=a-model-for-the-time-to-send-a-message>A model for the time to send a message
<a class=anchor href=#a-model-for-the-time-to-send-a-message>#</a></h2><p>We care about the total time it takes to send a message, our model is
a linear model which has two free parameters:</p><ol><li>$\alpha$, the message latency, measured in seconds;</li><li>$\beta$, the network bandwidth, measured in bytes/second.</li></ol><p>With this model, the time to send a message with $b$ bytes is</p><p>$$
T(b) = \alpha + \beta b
$$</p><h2 id=implementation>Implementation
<a class=anchor href=#implementation>#</a></h2><p>I provide a template in <a href=https://teaching.wence.uk/phys52015/code/mpi/ping-pong/ping-pong.c><code>mpi/ping-pong/ping-pong.c</code></a> that you can compile with <code>mpicc</code>. It
takes one argument, the size of the message (in bytes) to exchange.</p><p>You should implement the <code>ping_pong</code> function which should send a
message of the given size from rank 0 to rank 1, after which rank 1
should send the same message back to rank 0. Ensure that the code also
works with more than two processes (all other ranks should just do
nothing).</p><p>Add timing around the <code>ping_pong</code> call to determine how long it takes
to send these messages.</p><details><summary>Hint</summary><div class=markdown-inner><p>Use <a href=https://rookiehpc.com/mpi/docs/mpi_wtime.php><code>MPI_Wtime()</code></a> for
timing.</p><p>For small messages you will probably need to do many ping-pong
iterations in a loop to get accurate timings.</p></div></details><h2 id=experiment>Experiment
<a class=anchor href=#experiment>#</a></h2><blockquote class=exercise><h3>Exercise</h3><span><p>Run your code on the Hamilton compute nodes for a range of messages
sizes from one byte to 1MB.</p><p>Produce a plot of the time to send a message as a function of message
size.</p><p>Using
<a href=https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html><code>numpy.polyfit</code></a>
(or your favourite linear regression scheme), fit our proposed model
to your data.</p><p>What values of $\alpha$ and $\beta$ do you get?</p></span></blockquote><blockquote class=question><h3>Question</h3><span><p>Perform the same experiment, but this time, place the two processes on
<em>different</em> Hamilton compute nodes. Do you observe a difference in the
performance?</p><p>To do this, you&rsquo;ll need to write a SLURM batch script that specifies</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-sh data-lang=sh><span style=color:#75715e># Two nodes</span>
<span style=color:#75715e>#SBATCH --nodes=2</span>
<span style=color:#75715e># One process per node</span>
<span style=color:#75715e>#SBTACH --ntasks-per-node=1</span>
</code></pre></div></span></blockquote><h2 id=advanced-variability>Advanced: variability
<a class=anchor href=#advanced-variability>#</a></h2><blockquote class="book-hint info"><span>This section is optional, but possibly interesting.</span></blockquote><p>One thing that can affect performance of real MPI codes is the message
latency, and particularly if there is any variability. This might be
affected by other processes that happen to be using the network, or
our own code, or operating system level variability. We&rsquo;ll see if we
can observe any on Hamilton.</p><p>Modify your code so that rather than just timing many ping-pong
iterations, it records the time for each of the many iterations
separately.</p><p>Use this information to compute the mean ping-pong time, along with
the standard deviation and the minimum and maximum times.</p><details><summary>Hint</summary><div class=markdown-inner><p>You can allocate an array for your timing data with</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>int</span> <span style=color:#111>nrepeats</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span><span style=color:#111>;</span> <span style=color:#75715e>/* Or some approriate number */</span>
<span style=color:#00a8c8>double</span> <span style=color:#f92672>*</span><span style=color:#111>timing</span> <span style=color:#f92672>=</span> <span style=color:#111>malloc</span><span style=color:#111>(</span><span style=color:#111>nrepeats</span> <span style=color:#f92672>*</span> <span style=color:#00a8c8>sizeof</span><span style=color:#111>(</span><span style=color:#f92672>*</span><span style=color:#00a8c8>double</span><span style=color:#111>));</span>

<span style=color:#111>...;</span>
<span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>nrepeats</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span> <span style=color:#111>{</span>
   <span style=color:#111>start</span> <span style=color:#f92672>=</span> <span style=color:#111>MPI_Wtime</span><span style=color:#111>();</span>
   <span style=color:#111>ping_pong</span><span style=color:#111>(...);</span>
   <span style=color:#111>end</span> <span style=color:#f92672>=</span> <span style=color:#111>MPI_Wtime</span><span style=color:#111>();</span>
   <span style=color:#111>timing</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>end</span> <span style=color:#f92672>-</span> <span style=color:#111>start</span><span style=color:#111>;</span>
<span style=color:#111>}</span>
<span style=color:#111>...;</span> <span style=color:#75715e>/* Compute statistics */</span>

<span style=color:#111>free</span><span style=color:#111>(</span><span style=color:#111>timing</span><span style=color:#111>);</span> <span style=color:#75715e>/* Don&#39;t forget to release the memory! */</span>
</code></pre></div></div></details><p>Produce a plot of these data, using the standard deviation as error
bars and additionally showing the minimum and maximum times as
outliers.</p><blockquote class=question><h3>Question</h3><span><p>What, if any, variability do you observe?</p><p>Does it change if you move from a single compute node to two nodes?</p></span></blockquote><h3 id=network-contention>Network contention
<a class=anchor href=#network-contention>#</a></h3><p>Finally, we&rsquo;ll look at whether having more messages &ldquo;in flight&rdquo; at
once effects performance.</p><p>Rather than running with two processes, you should run with full
compute nodes (24 processes per node).</p><p>Modify your ping-pong code so that all ranks participate in pairwise
messaging.</p><p>Divide the processes into a &ldquo;bottom&rdquo; and &ldquo;top&rdquo; half. Suppose we are
using <code>size</code> processes in total. Processes with <code>rank &lt; size/2</code> are in
the &ldquo;bottom&rdquo; half, the remainder are in the &ldquo;top&rdquo; half.</p><p>A process in the bottom half should send a message to its matching
pair in the top half (<code>rank + size/2</code>), that process should then
return the message (to <code>rank - size/2</code>).</p><p>Again measure time and variability, and produce a plot.</p><blockquote class=question><h3>Question</h3><span><p>Do the results change from previously?</p><ol><li>When using one compute node?</li><li>When using two?</li><li>When using four?</li></ol></span></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/commit/a92175dd2301afd2496883b64d45a49391aeef3a title="Last modified by Lawrence Mitchell | December 2, 2020" target=_blank rel=noopener><img src=/phys52015/svg/calendar.svg class=book-icon alt=Calendar>
<span>December 2, 2020</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/edit/main/site/content/exercises/mpi-ping-pong.md target=_blank rel=noopener><img src=/phys52015/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence@wence.uk>Lawrence Mitchell</a>, <a href="https://www.dur.ac.uk/physics/staff/profiles/?mode=staff&id=16712">Christian Arnold</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/phys52015/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div></main></body></html>