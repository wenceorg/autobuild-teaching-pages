<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Avoiding data-races in updates to shared memory #  In this exercise, we&rsquo;ll use the synchronisation constructs we encountered when looking at OpenMP collectives to implement different approaches to combining the partial sums in a reduction.
We&rsquo;ll then also benchmark the performance of the different approaches to see if there are any differences
Template code and benchmarking #  In openmp-snippets/reduction-template.c is some code that times how long it takes to run the reduction."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="OpenMP: synchronisation"><meta property="og:description" content="Avoiding data-races in updates to shared memory #  In this exercise, we&rsquo;ll use the synchronisation constructs we encountered when looking at OpenMP collectives to implement different approaches to combining the partial sums in a reduction.
We&rsquo;ll then also benchmark the performance of the different approaches to see if there are any differences
Template code and benchmarking #  In openmp-snippets/reduction-template.c is some code that times how long it takes to run the reduction."><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/phys52015/exercises/openmp-reduction/"><meta property="article:modified_time" content="2020-11-19T15:46:42+00:00"><title>OpenMP: synchronisation | PHYS52015 – Introduction to HPC</title><link rel=manifest href=/phys52015/manifest.json><link rel=icon href=/phys52015/favicon.png type=image/x-icon><link rel=stylesheet href=/phys52015/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/phys52015/logo.svg alt=Logo><h2><a href=/phys52015>PHYS52015 – Introduction to HPC</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/phys52015/setup/remote/>Remote editing/development</a></li><li><a href=/phys52015/setup/hamilton-quickstart/>Hamilton access & quickstart</a></li><li><a href=/phys52015/setup/byod/>Local setup</a></li><li><a href=/phys52015/setup/configuration/>ssh configuration</a></li><li><a href=/phys52015/setup/unix/>Unix resources</a></li></ul></li><li><span>Exercises</span><ul><li><a href=/phys52015/exercises/hello/>Parallel Hello World</a></li><li><a href=/phys52015/exercises/vectorisation-loop/>Vectorisation: loops with conditionals</a></li><li><a href=/phys52015/exercises/vectorisation-stencil/>Vectorisation: stencils</a></li><li><a href=/phys52015/exercises/openmp-loop/>OpenMP: parallel loops</a></li><li><a href=/phys52015/exercises/openmp-stencil/>OpenMP: stencils</a></li><li><a href=/phys52015/exercises/openmp-reduction/ class=active>OpenMP: synchronisation</a></li><li><a href=/phys52015/exercises/mpi-pi/>MPI: Calculating π</a></li><li><a href=/phys52015/exercises/mpi-stencil/>MPI: halo exchanges</a></li></ul></li><li><span>Notes</span><ul><li><a href=/phys52015/notes/introduction/>Introduction and motivation</a></li><li><span>Theory & concepts</span><ul><li><a href=/phys52015/notes/theory/scaling-laws/>Parallel scaling laws</a></li><li><a href=/phys52015/notes/theory/hardware-parallelism/>Parallelism in hardware: an overview</a></li><li><a href=/phys52015/notes/theory/concepts/>Parallel patterns</a></li></ul></li><li><a href=/phys52015/notes/vectorisation/>Vectorisation</a><ul><li><a href=/phys52015/notes/vectorisation/compiler/>Compiler autovectorisation</a></li></ul></li><li><a href=/phys52015/notes/openmp/>OpenMP</a><ul><li><a href=/phys52015/notes/openmp/intro/>What is OpenMP?</a></li><li><a href=/phys52015/notes/openmp/loop-parallelism/>Loop parallelism</a></li><li><a href=/phys52015/notes/openmp/collectives/>Collectives</a></li></ul></li><li><a href=/phys52015/notes/mpi/>MPI</a><ul><li><a href=/phys52015/notes/mpi/point-to-point/>Point-to-point messaging in MPI</a></li><li><a href=/phys52015/notes/mpi/mpi-collectives/>Collectives</a></li><li><a href=/phys52015/notes/mpi/mpi-advanced/>Advanced topics</a></li></ul></li></ul></li><li><a href=/phys52015/coursework/>Coursework: parallel dense linear algebra</a><ul></ul></li><li><a href=/phys52015/resources/>Further resources</a></li><li><a href=/phys52015/acknowledgements/>Acknowledgements</a></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/phys52015/svg/menu.svg class=book-icon alt=Menu></label>
<strong>OpenMP: synchronisation</strong>
<label for=toc-control></label></div></header><article class=markdown><h1 id=avoiding-data-races-in-updates-to-shared-memory>Avoiding data-races in updates to shared memory
<a class=anchor href=#avoiding-data-races-in-updates-to-shared-memory>#</a></h1><p>In this exercise, we&rsquo;ll use the synchronisation constructs we
encountered when looking at <a href=https://teaching.wence.uk/phys52015/notes/openmp/collectives/>OpenMP collectives</a> to implement different approaches to combining
the partial sums in a reduction.</p><p>We&rsquo;ll then also benchmark the performance of the different approaches
to see if there are any differences</p><h2 id=template-code-and-benchmarking>Template code and benchmarking
<a class=anchor href=#template-code-and-benchmarking>#</a></h2><p>In <a href=https://teaching.wence.uk/phys52015/code/openmp-snippets/reduction-template.c><code>openmp-snippets/reduction-template.c</code></a> is some code that times
how long it takes to run the reduction.</p><p>You can select the length of the vector to compute the dot product of
by passing a size on the commandline. For example, after compiling with</p><pre><code>$ icc -qopenmp -o reduction-template reduction-template.c
</code></pre><p>You can run, for example, with</p><pre><code>$ OMP_NUM_THREADS=2 ./reduction-template 1000000
</code></pre><p>The implementation of the parallel reduction is left up to you.</p><p>You should implement a correct reduction using the four different
approaches listed in the code:</p><ol><li>&ldquo;By hand&rdquo;, using the same kind of approach as in
<code>openmp-snippets/reduction-hand.c</code>;</li><li>Using an atomic directive to protect the shared updates;</li><li>Using a critical section to protect the shared updates;</li><li>Using the reduction clause on a parallel loop.</li></ol><blockquote class=exercise><h3>Exercise</h3><span><p>For each of the approaches, benchmark the time it takes to run the
reduction for a vector with 1 billion entries across a range of
threads, from one up to 48 threads.</p><p>Produce plots of the <a href=https://teaching.wence.uk/phys52015/notes/theory/scaling-laws/>parallel scaling</a>
and parallel efficiency of the different approaches.</p></span></blockquote><blockquote class=question><h3>Question</h3><span><p>Which approach works best?</p><p>Which approach works worst?</p><p>Do you observe perfect scalability? That is, is the speedup linear in
the number of threads?</p></span></blockquote><h2 id=more-details-thread-placement>More details: thread placement
<a class=anchor href=#more-details-thread-placement>#</a></h2><p>The Hamilton compute nodes are dual-socket. That is, they have two
chips in a single motherboard, each with its own memory attached.
Although OpenMP treats this logically as a single piece of shared
memory, the performance of the code depends on where the memory
accessed is relative to where the threads are.</p><p>We will now briefly investigate this.</p><p>OpenMP exposes some extra environment variables that control where
threads are physically placed on the hardware. We&rsquo;ll look at how these
affect the performance of our reduction.</p><p>Use the implementation with the reduction clause, we hope it is the
most efficient!</p><p>The relevant environment variables for controlling placement are
<code>OMP_PROC_BIND</code> and <code>OMP_PLACES</code>. We need to set <code>OMP_PLACES=cores</code></p><pre><code>$ export OMP_PLACES=cores
</code></pre><details><summary>Hint</summary><div class=markdown-inner>Don&rsquo;t forget to do this in your submission script.</div></details><blockquote class=exercise><h3>Exercise</h3><span><p>We&rsquo;ll now look at the difference in scaling performance when using
different values for <code>OMP_PROC_BIND</code>.</p><p>As before, run with a vector with 1 billion entries, from one to 48
threads, and produce scaling plots.</p><p>First, use</p><pre><code>OMP_PROC_BIND=close
</code></pre><p>This places threads on cores that are physically close to one another,
first filling up the first socket, and then the second.</p><p>Compare this to results obtained with</p><pre><code>OMP_PROC_BIND=spread
</code></pre><p>This spreads threads out between cores, thread0 to the first socket,
thread 1 to the second, and so on. I think this is the default setting
when using the Intel OpenMP library.</p><p>Which approach works better? Is there a difference at all?</p></span></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/commit/600c156c8020f60b871dafde4a5eac91a03a7474 title="Last modified by Lawrence Mitchell | November 19, 2020" target=_blank rel=noopener><img src=/phys52015/svg/calendar.svg class=book-icon alt=Calendar>
<span>November 19, 2020</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/phys52015/edit/main/site/content/exercises/openmp-reduction.md target=_blank rel=noopener><img src=/phys52015/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence@wence.uk>Lawrence Mitchell</a>, <a href="https://www.dur.ac.uk/physics/staff/profiles/?mode=staff&id=16712">Christian Arnold</a> & <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/phys52015/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div></main></body></html>