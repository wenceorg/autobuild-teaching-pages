<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Introduction on PHYS52015 – Introduction to HPC</title><link>https://teaching.wence.uk/phys52015/</link><description>Recent content in Introduction on PHYS52015 – Introduction to HPC</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://teaching.wence.uk/phys52015/index.xml" rel="self" type="application/rss+xml"/><item><title>Compiler autovectorisation</title><link>https://teaching.wence.uk/phys52015/notes/vectorisation/compiler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/vectorisation/compiler/</guid><description>Obtained vectorised code # If we write code that we would like to be vectorised, we have multiple different options available to us on how to obtain it. We&amp;rsquo;ll first list some approaches, briefly detail their strengths and weaknesses, and then go into more detail on the approach that we&amp;rsquo;ll be using in this course.
First, some general advice. It is tempting to think of vectorisation as an optimisation that we can apply locally to our code.</description></item><item><title>Introduction and motivation</title><link>https://teaching.wence.uk/phys52015/notes/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/introduction/</guid><description>Introduction # This course provides a brief introduction to parallel computing. It focuses on those paradigms that are prevalent in scientific computing in academia. There are other parallel programming models which we will mention in passing where appropriate.
Task Sign up for a Hamilton account if you don&amp;rsquo;t already have one. If you have a COSMA account, you can use that instead, and need not sign up for Hamilton access (although you may wish to anyway).</description></item><item><title>Parallel Hello World</title><link>https://teaching.wence.uk/phys52015/exercises/hello/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/hello/</guid><description>Hello, World! # As with every programming course, the first thing we will do is compile and run a &amp;ldquo;Hello world&amp;rdquo; program. Actually we&amp;rsquo;ll do three. The goal of this is to familiarise you with the module system on Hamilton, as well as how to compile code. So take a look at the quickstart guide if you haven&amp;rsquo;t already.
A serial version # Log in to Hamilton/COSMA load the relevant compiler modules</description></item><item><title>Point-to-point messaging in MPI</title><link>https://teaching.wence.uk/phys52015/notes/mpi/point-to-point/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/mpi/point-to-point/</guid><description>Pairwise message exchange # The simplest form of communication in MPI is a pairwise exchange of a message between two processes.
In MPI, communication via messages is two-sided1. That is, for every message one process sends, there must be a matching receive call by another process.
Cartoon of sending a message between two processes
We need to fill in some details
How will we describe &amp;ldquo;data&amp;rdquo; How will we identify processes How will the receiver know which message to put where?</description></item><item><title>What is OpenMP?</title><link>https://teaching.wence.uk/phys52015/notes/openmp/intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/openmp/intro/</guid><description>What is OpenMP # OpenMP is a standardised API for programming shared memory computers (and more recently GPUs) using threading as the programming paradigm. It supports both data-parallel shared memory programming (typically for parallelising loops) and task parallelism. We&amp;rsquo;ll see some examples later.
In recent years, it has also gained support for some vector-based parallelism.
Using OpenMP # OpenMP is implemented as a set of extensions for C, C++, and Fortran.</description></item><item><title>Loop parallelism</title><link>https://teaching.wence.uk/phys52015/notes/openmp/loop-parallelism/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/openmp/loop-parallelism/</guid><description>OpenMP loop parallelism # With a parallel region and identification of individual threads, we can actually parallelise loops &amp;ldquo;by hand&amp;rdquo;.
Suppose we wish to divide a loop approximately equally between all the threads, by assigning consecutive blocks of the loop iterations to consecutive threads.
Distribution of 16 loop iterations across five threads.
Notice here that the number of iterations is not evenly divisible by the number of threads, so we&amp;rsquo;ve assigned one extra iteration to thread0.</description></item><item><title>Non-blocking point-to-point messaging</title><link>https://teaching.wence.uk/phys52015/notes/mpi/point-to-point-nb/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/mpi/point-to-point-nb/</guid><description>Non-blocking messages # As well as the blocking point to point messaging we saw last time, MPI also offers non-blocking versions.
These functions all return immediately, and provide a &amp;ldquo;request&amp;rdquo; object that we can then either wait for completion with or inspect to check if the message has been sent/received.
The function signatures for MPI_Isend and MPI_Irecv are:
int MPI_Isend(const void *buffer, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm, MPI_Request *request); int MPI_Irecv(void *buffer, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm, MPI_Request *request); Notice how the send gets an extra output argument (the request), and the receive loses the MPI_Status output argument and gains a request output argument.</description></item><item><title>Parallel scaling laws</title><link>https://teaching.wence.uk/phys52015/notes/theory/scaling-laws/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/theory/scaling-laws/</guid><description>Scaling laws (models) # Suppose we have a simulation that has been parallelised. What should we expect of its performance when we run it in parallel? Answering this question will allow us to determine an appropriate level of parallelism to use when running the simulation. It can also help us determine how much effort to put into parallelising a serial code, or improving an existing parallel code.
Some simple examples1 # The type of parallelism we will cover in this course is that where we, as programmers, are explicitly in control of what is going on.</description></item><item><title>Vectorisation: loops with conditionals</title><link>https://teaching.wence.uk/phys52015/exercises/vectorisation-loop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/vectorisation-loop/</guid><description>Vectorisation of a loop with conditionals # We&amp;rsquo;ll be running these exercises on Hamilton or COSMA, so remind yourself of how to log in and transfer code if you need to.
Some expensive calculations # In this exercise, we&amp;rsquo;re going to look at the ability of compilers to vectorise a loop in the presence of conditionals inside the loop body. The idea is to observe, and understand, what patterns permit (or do not permit) vectorisation.</description></item><item><title>Parallelism in hardware: an overview</title><link>https://teaching.wence.uk/phys52015/notes/theory/hardware-parallelism/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/theory/hardware-parallelism/</guid><description>Supercomputer architecture # When thinking about the implementation of parallel programming, it is helpful to have at least a high-level idea of what kinds of parallelism are available in the hardware. The different levels of parallelism available in the hardware then map onto (sometimes) different programming models.
Distributed memory parallelism # Modern supercomputers are all massively parallel, distributed memory, systems. That is, they consist of many processors linked by some network, but do not share a single memory system.</description></item><item><title>Vectorisation: stencils</title><link>https://teaching.wence.uk/phys52015/exercises/vectorisation-stencil/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/vectorisation-stencil/</guid><description>Vectorisation of a simple stencil # We&amp;rsquo;ll be running these exercises on Hamilton or COSMA, so remind yourself of how to log in and transfer code if you need to.
Blurring an image # One can blur or smooth the edges of an image by convolving the image with a normalised box kernel. Every output pixel \( g_{k, l} \) is created from the mean of the input image pixel \(f _{k, l}\) and its eight neighbours.</description></item><item><title>Further resources</title><link>https://teaching.wence.uk/phys52015/resources/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/resources/</guid><description>Other reading/resources # There are many excellent textbooks and tutorials on parallel programming. Here are some I like, with brief commentary.
Victor Eijkhout, Introduction to High Performance Computing. This is a gentle, and quite comprehensive, introduction to parallel computing. The author has a separate, in progress, book which contains more details on OpenMP and MPI programming models.
Georg Hager &amp;amp; Gerhard Wellein, Introduction to High Performance Computing for Scientists and Engineers, also available through the Durham library.</description></item><item><title>OpenMP: parallel loops</title><link>https://teaching.wence.uk/phys52015/exercises/openmp-loop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/openmp-loop/</guid><description>Parallelisation of a simple loop # As usual, we&amp;rsquo;ll be running these exercises on Hamilton or COSMA, so remind yourself of how to log in and transfer code if you need to.
Obtaining the code # We&amp;rsquo;re going to use the same add_numbers code as we did in the previous vectorisation exercise. You should undo your edits from that exercise. If you can&amp;rsquo;t remember what you changed just download and unpack the code again.</description></item><item><title>Parallel patterns</title><link>https://teaching.wence.uk/phys52015/notes/theory/concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/theory/concepts/</guid><description>Overview of parallel patterns # So far, we&amp;rsquo;ve seen that to run large simulations, and exploit all of the hardware available to us on supercomputers (but even on laptops and phones), we will need to use parallelism of some kind.
We&amp;rsquo;ve also looked at the levels of parallelism exposed by modern hardware, and noted that there are effectively three levels.
Now we&amp;rsquo;re going to look at the types of parallelism (or parallel patterns) that we might encounter in software.</description></item><item><title>Collectives</title><link>https://teaching.wence.uk/phys52015/notes/openmp/collectives/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/openmp/collectives/</guid><description>OpenMP collectives # So far we&amp;rsquo;ve seen how we can create thread teams using #pragma omp parallel and distribute work in loops between members of the team by using #pragma omp for.
Now we&amp;rsquo;ll look at what we need to do if we need to communicate between threads.
Reductions # Remember that the OpenMP programming model allows communication between threads by using shared memory. If some piece of memory is shared in a parallel region then every thread in the team can both read and write to it.</description></item><item><title>OpenMP: stencils</title><link>https://teaching.wence.uk/phys52015/exercises/openmp-stencil/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/openmp-stencil/</guid><description>Revisiting the stencil exercise # We&amp;rsquo;ll revisit the stencil computation that we used when looking at vectorisation. This time, we&amp;rsquo;re going to look at parallelisation with OpenMP.
Obtaining and compiling the code # If you already downloaded the code from the previous exercise you already have the code, otherwise get the tar archive and unpack it.
This time we&amp;rsquo;ll be working in the openmp subdirectory:
$ cd blur_image/openmp/ $ ls Makefile filters.</description></item><item><title>Remote editing/development</title><link>https://teaching.wence.uk/phys52015/setup/remote/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/setup/remote/</guid><description>Remote development and managing changes # Remote editing # Editing in a terminal is somewhat painful, and laboriously copying files back and forth between your local machine and Hamilton is error-prone.
A better approach is to use a combination of version control and remote editing. I provide brief instructions for VS Code which is a popular editor, and very brief instructions for Emacs.
VS Code # VS Code has an extension for remote editing which allows you to edit files on remote systems as if you&amp;rsquo;re running locally.</description></item><item><title>OpenMP: synchronisation</title><link>https://teaching.wence.uk/phys52015/exercises/openmp-reduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/openmp-reduction/</guid><description>Avoiding data-races in updates to shared memory # In this exercise, we&amp;rsquo;ll use the synchronisation constructs we encountered when looking at OpenMP collectives to implement different approaches to combining the partial sums in a reduction.
We&amp;rsquo;ll then also benchmark the performance of the different approaches to see if there are any differences
Template code and benchmarking # In openmp-snippets/reduction-template.c is some code that times how long it takes to run the reduction.</description></item><item><title>MPI: messages round a ring</title><link>https://teaching.wence.uk/phys52015/exercises/mpi-ring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/mpi-ring/</guid><description>Sending messages around a ring # In this exercise we&amp;rsquo;ll write a simple form of global reduction. We will set up the processes in a ring (so each process has a left and right neighbour) and each process should initialise a buffer to its rank.
To compute a global summation a simple method is to rotate each piece of data all the way round the ring: at each step, a process receives from the left and sends to the right.</description></item><item><title>Collectives</title><link>https://teaching.wence.uk/phys52015/notes/mpi/mpi-collectives/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/mpi/mpi-collectives/</guid><description>MPI collectives # Motivation
What is it
How does it work
Some API
Collectives basics
Where next?</description></item><item><title>MPI: Calculating π</title><link>https://teaching.wence.uk/phys52015/exercises/mpi-pi/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/mpi-pi/</guid><description>Simple MPI parallelism # In this exercise we&amp;rsquo;re going to compute an approximation to the value of π using a simple Monte Carlo method. We do this by noticing that if we randomly throw darts at a square, the fraction of the time they will fall within the incircle approaches π.
Consider a square with side-length \(2r\) and an inscribed circle with radius \(r\).
Square with inscribed circle</description></item><item><title>MPI: halo exchanges</title><link>https://teaching.wence.uk/phys52015/exercises/mpi-stencil/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/mpi-stencil/</guid><description>Data parallel stencil computations # In this exercise we&amp;rsquo;re going to explore an MPI-parallelisation of the image blurring exercise.
The natural thing to do here is to divide the work up between processes (much as we did for the OpenMP version).
Let&amp;rsquo;s have a look at what this would look like.
Decomposition of a 2D image into nine pieces
Remember that in MPI, each process has its own memory space.</description></item><item><title>MPI: ping-pong latency</title><link>https://teaching.wence.uk/phys52015/exercises/mpi-ping-pong/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/exercises/mpi-ping-pong/</guid><description>Measuring point-to-point message latency with ping-pong # In this exercise we will write a simple code that does a message ping-pong: sending a message back and forth between two processes.
We can use this to measure both the latency and bandwidth of the network on our supercomputer. Which are both important measurements when we&amp;rsquo;re looking at potential parallel performance: they help us to decide if our code is running slowly because of our bad choices, or limitations in the hardware.</description></item><item><title>Acknowledgements</title><link>https://teaching.wence.uk/phys52015/acknowledgements/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/acknowledgements/</guid><description>Acknowledgements # Some of the exercises and examples, where noted, are adapted from Victor Eijkhout&amp;rsquo;s introductory high performance computing textbook.
Some of the material is adapted from a previous version of the course taught by Tobias Weinzierl at Durham.</description></item><item><title>Advanced topics</title><link>https://teaching.wence.uk/phys52015/notes/mpi/mpi-advanced/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/notes/mpi/mpi-advanced/</guid><description>Some pointers to advanced features # Communicator manipulation
MPI-IO
One-sided communication
Non-blocking collectives
Neighbourhood collectives</description></item><item><title>Hamilton access &amp; quickstart</title><link>https://teaching.wence.uk/phys52015/setup/hamilton-quickstart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/setup/hamilton-quickstart/</guid><description>Accessing Durham Supercomputing facilities # Most of the exercises in this course will require that you use one of Durham&amp;rsquo;s supercomputing systems. This will either be Hamilton, or (for some Physics students) COSMA. You don&amp;rsquo;t automatically get an account on Hamilton, so you&amp;rsquo;ll need to register for one.
Access to Hamilton # For many of the exercises in the course, we will be using the Hamilton supercomputer. You should obtain an account on Hamilton by following their instructions.</description></item><item><title>Local setup</title><link>https://teaching.wence.uk/phys52015/setup/byod/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/setup/byod/</guid><description>Running the exercises on your own hardware # Although you don&amp;rsquo;t need to, you might find it useful or convenient to run the exercises on your own computer. To do so, you&amp;rsquo;ll need to obtain appropriate compilers and MPI implementations.
For more general information on getting a development toolchain setup, the Faculty of Natural Sciences at Imperial College have prepared some useful guides
MacOS # Open source tools # I recommend homebrew for installation of packages on MacOS.</description></item><item><title>ssh configuration</title><link>https://teaching.wence.uk/phys52015/setup/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/setup/configuration/</guid><description>ssh tips &amp;amp; tricks # Setting up simpler logins # It can be tedious to remember to type long login commands every time when logging in via ssh to hamilton. I therefore recommend that you set up an ssh config file.
Additionally, you might also want to set up ssh keys for passwordless login.
The ssh-config configuration file # Mac/Linux When you run ssh it reads a configuration file at $HOME/.</description></item><item><title>Unix resources</title><link>https://teaching.wence.uk/phys52015/setup/unix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://teaching.wence.uk/phys52015/setup/unix/</guid><description>Using Unix-like systems for computational science # This course presupposes some level of familiarity with commandline interfaces. In case you need a quick refresher, I recommend the material produced by the Software Carpentry project.
They have a number of useful lessons and materials providing introductory training on how to do things like use the Unix shell, version control with git, and some introductory programming and plotting in Python. They are good place to start for a gentle introduction to these topics.</description></item></channel></rss>