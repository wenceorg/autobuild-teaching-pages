<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="An overview of memory hierarchies #  Reduction benchmark #  In exercise 1 you looked at the performance of a vectorised and non-vectorised version of a very simple loop computing the sum of an array of floating point numbers.
In doing so, you produced a plot of the performance (in terms of floating point throughput) as a function of array size. You should have observed something similar to that shown here."><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="The memory hierarchy"><meta property="og:description" content="An overview of memory hierarchies #  Reduction benchmark #  In exercise 1 you looked at the performance of a vectorised and non-vectorised version of a very simple loop computing the sum of an array of floating point numbers.
In doing so, you produced a plot of the performance (in terms of floating point throughput) as a function of array size. You should have observed something similar to that shown here."><meta property="og:type" content="article"><meta property="og:url" content="https://teaching.wence.uk/comp52315/notes/memory/"><meta property="article:modified_time" content="2020-11-18T19:21:56+00:00"><title>The memory hierarchy | COMP52315 – Performance Engineering</title><link rel=manifest href=/comp52315/manifest.json><link rel=icon href=/comp52315/favicon.png type=image/x-icon><link rel=stylesheet href=/comp52315/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/comp52315/logo.svg alt=Logo><h2><a href=/comp52315>COMP52315 – Performance Engineering</a></h2></div><ul><li><span>Administrivia</span><ul><li><a href=/comp52315/setup/contact/>Contact details</a></li><li><a href=/comp52315/setup/hamilton/>Hamilton accounts</a></li><li><a href=/comp52315/setup/configuration/>ssh configuration</a></li><li><a href=/comp52315/setup/unix/>Unix resources</a></li></ul></li><li><span>Exercises</span><ul><li><a href=/comp52315/exercises/exercise01/>Exercise 1: sum reductions</a></li><li><a href=/comp52315/exercises/exercise02/>Exercise 2: caches</a></li><li><a href=/comp52315/exercises/exercise03/>Exercise 3: memory bandwidth</a></li><li><a href=/comp52315/exercises/exercise04/>Exercise 4: roofline analysis</a></li><li><a href=/comp52315/exercises/exercise05/>Exercise 5: models and measurements</a></li><li><a href=/comp52315/exercises/exercise06/>Exercise 6: profiling</a></li><li><a href=/comp52315/exercises/exercise07/>Exercise 7: loop tiling matrix transpose</a></li><li><a href=/comp52315/exercises/exercise08/>Exercise 8: loop tiling matrix-matrix multiplication</a></li><li><a href=/comp52315/exercises/exercise09/>Exercise 9: compiler feedback</a></li><li><a href=/comp52315/exercises/exercise10/>Exercise 10: stencil layer conditions</a></li></ul></li><li><span>Notes</span><ul><li><a href=/comp52315/notes/introduction/>Introduction</a></li><li><a href=/comp52315/notes/memory/ class=active>The memory hierarchy</a></li><li><a href=/comp52315/notes/roofline/>Performance models: roofline</a></li></ul></li><li><a href=/comp52315/coursework/>Coursework: fast finite elements</a><ul></ul></li><li><a href=/comp52315/acknowledgements/>Acknowledgements & further reading</a></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/comp52315/svg/menu.svg class=book-icon alt=Menu></label>
<strong>The memory hierarchy</strong>
<label for=toc-control></label></div></header><article class=markdown><h1 id=an-overview-of-memory-hierarchies>An overview of memory hierarchies
<a class=anchor href=#an-overview-of-memory-hierarchies>#</a></h1><h2 id=reduction-benchmark>Reduction benchmark
<a class=anchor href=#reduction-benchmark>#</a></h2><p>In <a href=https://teaching.wence.uk/comp52315/exercises/exercise01/>exercise 1</a> you looked at the
performance of a vectorised and non-vectorised version of a very
simple loop computing the sum of an array of floating point numbers.</p><p>In doing so, you produced a plot of the performance (in terms of
floating point throughput) as a function of array size. You should
have observed something similar to that shown here.</p><p>FIXME: add figure</p><p>We see that the <abbr title="Single Instruction Multiple
Data">SIMD</abbr> (vectorised) code has four distinct performance
plateaus as a function of the array size, whereas the scalar code has
only two.</p><p>On this hardware (Broadwell), the chip can issue up to one <code>ADD</code>
(scalar or vector) per cycle. The peak clock speed is 2.9GHz. So the
peak scalar throughput of addition is 2.9GFlops/s, while the peak
vector throughput is \(2.9 \times 8 = 23.2\)GFlops/s.</p><p>We can see that the vector code achieves peak throughput for small
vectors, but not large ones. Why is this?</p><p>Remember that as well as thinking about the <a href=https://teaching.wence.uk/comp52315/notes/introduction/#resource-bottleneck-instruction-throughput>primary resource</a> of
instruction throughput, we also need to consider whether <a href=https://teaching.wence.uk/comp52315/notes/introduction/#resource-bottleneck-data-transfers>data
transfers</a> are
producing the bottleneck. For this, we need to consider the memory
hierarchy.</p><h2 id=memory-hierarchy>Memory hierarchy
<a class=anchor href=#memory-hierarchy>#</a></h2><p>In the von Neumann model, program code and data must be transferred
from memory to the CPU (and back again). To speed up computation we can
increase the speed at which instructions execute. We can also reduce
the time it takes to <em>move</em> data between the memory and the CPU.</p><p>In an ideal world, to process lots of data very fast, we would have
<em>large</em> (in terms of storage) and <em>fast</em> (in terms of transfer speed)
memory. Unfortunately, physics gets in the way, and we can pick one of</p><ol><li><em>small</em> and <em>fast</em></li><li><em>large</em> and <em>slow</em></li></ol><p>In fact, there is a sliding scale here, as we make the storage
capacity smaller we can make the memory faster, and vice versa.</p><p>We have something close to the following picture</p><figure><img src=https://teaching.wence.uk/comp52315/images/manual/cachesketch.png alt="As memory gets larger, it must become slower, both in latency and bandwidth" width=70%><figcaption><p>As memory gets larger, it must become slower, both in latency and bandwidth</p></figcaption></figure><p>To explore these latencies in more depth (and see how they&rsquo;ve changed
over time), see <a href=https://colin-scott.github.io/personal_website/research/interactive_latency.html>latency numbers every programmer should
know</a>.</p><h2 id=caches>Caches
<a class=anchor href=#caches>#</a></h2><p>Having identified the high level problem that we can&rsquo;t make large,
fast memory, what can chip designers do about it? The answer (on CPUs
at least) is <em>caches</em>.</p><p>The idea is that we add a hierarchy of small, fast memory. These keep
a copy of <em>frequently used</em> data and are used to speed up access.
Except in certain special cases, it&rsquo;s not possible to know <em>which</em>
data will be used frequently. As a consequence, caches rely on a
<em>principle of locality</em>.</p><p>FIXME: add details on caches</p><h2 id=measurement>Measurement
<a class=anchor href=#measurement>#</a></h2><p>As well as using
<a href=https://github.com/RRZE-HPC/likwid/wiki/Likwid-Bench>likwid-bench</a>
to measure floating point throughput of some simple loops, we can also
use it to measure memory bandwidth. In <a href=https://teaching.wence.uk/comp52315/exercises/exercise02/>exercise 2</a> you should do this to determine the cache
and main memory bandwidth on the Hamilton cores. We will use this to
construct a predictive model of the floating point throughput of the
reduction from <a href=https://teaching.wence.uk/comp52315/exercises/exercise01/>exercise 1</a>.</p><p>FIXME: add results</p><h2 id=a-predictive-model-for-reductions>A predictive model for reductions
<a class=anchor href=#a-predictive-model-for-reductions>#</a></h2><p>Let us remind ourselves of the code we want to predict the performance
of</p><div class="book-columns flex flex-wrap"><div class="flex-even markdown-inner"><h4 id=c-code>C code</h4><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>float</span> <span style=color:#75af00>reduce</span><span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>N</span><span style=color:#111>,</span> 
    <span style=color:#00a8c8>const</span> <span style=color:#00a8c8>double</span> <span style=color:#f92672>*</span><span style=color:#00a8c8>restrict</span> <span style=color:#111>a</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>float</span> <span style=color:#111>c</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span>
  <span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span>
    <span style=color:#111>c</span> <span style=color:#f92672>+=</span> <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>];</span>
  <span style=color:#00a8c8>return</span> <span style=color:#111>c</span><span style=color:#111>;</span>
<span style=color:#111>}</span>
</code></pre></div></div><div class="flex-even markdown-inner"><h4 id=vectorised-pseudo-assembly>Vectorised pseudo-assembly</h4><pre><code>LOAD [r1.0, ..., r1.7] ← 0
i ← 0
loop:
  LOAD [r2.0, ..., r2.7] ← [a[i], ..., a[i+7]]
  ADD r1 ← r1 + r2 ; SIMD ADD
  i ← i + 8
  if i &lt; N: loop
result ← r1.0 + r1.1 + ... + r1.7
</code></pre></div></div><p>The accumulation parameter <code>c</code> is held in a register. At each
iteration of the vectorised loop, we load eight elements of <code>a</code> into a
recond register. Since each <code>float</code> value takes 4 bytes, this means
that each iteration of the loop requires 32 bytes of data.</p><p>Recall that we can run one <code>ADD</code> per cycle. To keep up with the
addition, the memory movement must therefore deliver 32 bytes/cycle.</p><p>At 2.9GHz, this translates to a sustained load bandwidth of</p><p>$$
32 \text{bytes/cycle} \times 2.9 \times 10^9 \text{cycle/s} = 92.8\text{Gbyte/s}.
$$</p><p>Let&rsquo;s match this up with our measurements.</p><h3 id=l1-bandwidth>L1 bandwidth
<a class=anchor href=#l1-bandwidth>#</a></h3><p>The smallest (and fastest) cache is the level one (or L1) cache. On
this hardware, we observe a sustained load bandwidth of around
300Gbyte/s. Hence, when the data fit in L1 (less than 32KB), the speed
of executing the <code>ADD</code> instruction is the limit.</p><h3 id=l2-bandwidth>L2 bandwidth
<a class=anchor href=#l2-bandwidth>#</a></h3><p>The next level of cache is level two (L2). This provides around
80Gbyte/s or 27bytes/cycle. Since \( 27 &lt; 32 \), we can&rsquo;t reach the
floating point peak when the data fit in L2. The best we can hope for
is</p><p>$$
2.9 \times 8 \times \frac{27}{32} = 19.6\text{GFlops/s}.
$$</p><h3 id=l3-and-main-memory-bandwidth>L3 and main memory bandwidth
<a class=anchor href=#l3-and-main-memory-bandwidth>#</a></h3><p>We apply the same idea to the level three (L3) cache and main memory.
L3 provides around 36Gbyte/s or 12bytes/cycle. We obtain an upper
limit of</p><p>$$
2.9 \times 8 \times \frac{12}{32} = 8.7\text{GFlops/s}
$$</p><p>For main memory, the memory bandwidth is around 13Gbyte/s or
4.5bytes/cycle, and the peak is approximately 3.25GFlops/s.</p><p>Let&rsquo;s redraw our floating point throughput graph, this time annotating
it with these predicted performance limits.</p><p>We can see that this simple model does a pretty good job of predicting
the performance of our test code.</p><h2 id=stepping-back>Stepping back
<a class=anchor href=#stepping-back>#</a></h2><p>This idea of predicting performance based on resource limits is a
powerful one, and we will return to it through the rest of the course.</p><p>As a practice, see if you can come up with throughput limits for the
following piece of code</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-c data-lang=c><span style=color:#00a8c8>void</span> <span style=color:#75af00>stream_triad</span><span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>N</span><span style=color:#111>,</span> <span style=color:#00a8c8>float</span> <span style=color:#f92672>*</span> <span style=color:#00a8c8>restrict</span> <span style=color:#111>a</span><span style=color:#111>,</span>
                  <span style=color:#00a8c8>const</span> <span style=color:#00a8c8>float</span> <span style=color:#f92672>*</span> <span style=color:#00a8c8>restrict</span> <span style=color:#111>b</span><span style=color:#111>,</span>
                  <span style=color:#00a8c8>const</span> <span style=color:#00a8c8>float</span> <span style=color:#f92672>*</span> <span style=color:#00a8c8>restrict</span> <span style=color:#111>c</span><span style=color:#111>,</span>
                  <span style=color:#00a8c8>const</span> <span style=color:#00a8c8>float</span> <span style=color:#111>alpha</span><span style=color:#111>)</span>
<span style=color:#111>{</span>
  <span style=color:#00a8c8>for</span> <span style=color:#111>(</span><span style=color:#00a8c8>int</span> <span style=color:#111>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span><span style=color:#111>;</span> <span style=color:#111>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>N</span><span style=color:#111>;</span> <span style=color:#111>i</span><span style=color:#f92672>++</span><span style=color:#111>)</span>
    <span style=color:#111>a</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>b</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>]</span><span style=color:#f92672>*</span><span style=color:#111>alpha</span> <span style=color:#f92672>+</span> <span style=color:#111>c</span><span style=color:#111>[</span><span style=color:#111>i</span><span style=color:#111>];</span>
<span style=color:#111>}</span>
</code></pre></div><p>The Broadwell chips on Hamilton can execute up to two loads and one
store per cycle. To determine the floating point limit (assuming no
memory constraints) note that this operation perfectly matches a
&ldquo;fused multiply add&rdquo;</p><pre><code>a_i ← b_i * alpha + c_i
</code></pre><p>Which is implemented as a single instruction <code>FMA</code>. Broadwell chips
can execute up to two <code>FMA</code> instructions per cycle.</p><p>We will revisit this in a <a href=https://teaching.wence.uk/comp52315/exercises/exercise05/>later exercise</a>.</p><h2 id=scalable-and-saturating-resources>Scalable and saturating resources
<a class=anchor href=#scalable-and-saturating-resources>#</a></h2><p>Although most of the focus in this course is on single core
performance, it is worthwhile taking a little time to consider how
resource use changes when we involve more cores. All modern chips have
more than one core on them. Some of the resources on a chip are
therefore private to each core, and some are shared. In particular,
the floating point units are private: adding more cores increasing the
total floating point throughput. In contrast, the main memory is
shared between cores, so adding more cores <em>does not</em> increase the
memory bandwidth.</p><p>We can ask likwid, using
<a href=https://github.com/RRZE-HPC/likwid/wiki/likwid-topology><code>likwid-topology</code></a>
to provide us some information on the layout of the system we are
running on. It produces a schematic of the core and memory layout in
ASCII, similar to the diagram below</p><figure><img src=https://teaching.wence.uk/comp52315/images/manual/cacheschematic.png alt="Example layout of caches and memory for a 4 core system." width=70%><figcaption><p>Example layout of caches and memory for a 4 core system.</p></figcaption></figure><p>Although in this course we will spend most of our time focussing on
<em>single core</em> performance, in practice, most scientific computing
algorithms will be parallel.</p><p>To understand how parallelisation will affect the performance on real
hardware, we need to know if will be limited by a resource which is
scalable, or saturating.</p><div class="book-columns flex flex-wrap"><div class="flex-even markdown-inner"><h3 id=scalable-resources>Scalable resources</h3><p>These resources are private to each core/chip. For example, CPU cores
themselves are a <em>scalable</em> resource. Adding a second core doubles the
number of floating point operations we can perform.</p><p>As a consequence, if our code is limited by the floating point
throughput, adding more cores is a useful thing to do.</p><figure><img src=https://teaching.wence.uk/comp52315/images/auto/scalable-resource.png alt="Prototypical performance of a scalable resource" width=100%><figcaption><p>Prototypical performance of a scalable resource</p></figcaption></figure></div><div class="flex-even markdown-inner"><h3 id=saturatingshared-resources>Saturating/shared resources</h3><p>These resources are shared between cores. The typical example is main
memory bandwidth. In the diagram above, we see that the main memory
interface is shared between the four cores. This is typical for modern
CPUs.</p><p>On a single chip, if our code is limited by the main memory bandwidth,
adding more cores is <em>not</em> useful. Instead we would need to add
another chip (with another memory system).</p><figure><img src=https://teaching.wence.uk/comp52315/images/auto/saturating-resource.png alt="Prototypical performance of a saturating resource" width=100%><figcaption><p>Prototypical performance of a saturating resource</p></figcaption></figure></div></div><p>You should explore this on Hamilton in <a href=https://teaching.wence.uk/comp52315/exercises/exercise03/>exercise 3</a></p><h2 id=summary-challenges-for-writing-high-performance-code>Summary: challenges for writing high performance code
<a class=anchor href=#summary-challenges-for-writing-high-performance-code>#</a></h2><p>At a high level, the performance of an algorithm is dependent on:</p><ol><li>how many instructions are required to implement the algorithm;</li><li>how efficiently those instructions can be executed on a processor;</li><li>and what the runtime contribution of the required data transfers
is.</li></ol><p>Given an optimal <em>algorithm</em>, converting that to an optimal
<em>implementation</em> requires addressing all of these points in tandem.
This is made complicated by the complexity and parallelism of modern
hardware. A typical Intel server offers</p><ol><li>Socket-based parallelism: 1-4 CPUs on a typical motherboard;</li><li>Core-based parallelism: 4-32 cores in a typical CPU;</li><li>SIMD/Vectorisation: vector registers capable of holding 2-16 single;
precision elements on each core;</li><li>Superscalar execution: typically 2-8 instructions per cycle per core.</li></ol><p>To limit the scope to something reasonable, we will focus mainly on
the SIMD and superscalar parts of this picture. The rationale for this
is that we should aim for good single core performance <em>before</em>
looking at parallelism. If we don&rsquo;t, we might be led in the wrong
direction by a &ldquo;false&rdquo; idea of the performance limits.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/wenceorg/comp52315/commit/b5ba3b1fd2c7c435816f4e07b9c9590bb22cf414 title="Last modified by Lawrence Mitchell | November 18, 2020" target=_blank rel=noopener><img src=/comp52315/svg/calendar.svg class=book-icon alt=Calendar>
<span>November 18, 2020</span></a></div><div><a class="flex align-center" href=https://github.com/wenceorg/comp52315/edit/main/site/content//notes/memory.md target=_blank rel=noopener><img src=/comp52315/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash; <a href=mailto:lawrence@wence.uk>Lawrence Mitchell</a> and <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/comp52315/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div></main></body></html>